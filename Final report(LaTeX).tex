\documentclass{article}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{textcomp}
\usepackage{graphicx}

\title{
  \textbf{
    \huge Technical University of Munich \\ Campus Straubing \\ Faculty of Bioinformatics
      }\\
  \vspace{0,25cm}
  \huge Final report: Research internship \\ Applying Reinforcement Learning to \textit{Gym'} Lunar      Lander
  }
\author{Krystian Budkiewicz}
\date{Date: 01.12.2022}
\maketitle
\pagebreak

\begin{document}

%\tableofcontents
\newpage
\section*{Abstract}
This document is the final report summarizing my work during my 6-week research internship at the Faculty of Bioinformatics led by Prof. Dominik Grimm at TUM Campus Straubing. Under the assistance of PhD Student Jonathan Pirnay I was introduced into the basic concepts of Reinforcement Learning (RL) and Deep Q-Learning (DQL). After gaining knowdledge in this topics, I I programmed my own Deep Q-Algorithm, which main goal was to solve the environment of \textit{Lunar Lander}, i.e. achieving an average score of 200 in last 100 episodes.
%allowing usage of preexisting environments based of Atari 2600 games and other games used for testing DQN-Algorithms.
This document summarizes what I have learned about both Reinforcement Learning and Deep Q-Learning, and describes their implementation in \textit{Python} programming language.

\newpage
\section*{Introduction}
Sample text Sample text Sample text Sample text Sample text Sample text Sample text Sample text
Sample text Sample text Sample text Sample text Sample text Sample text Sample text Sample text
\\

\section*{Methods}
The Deep Q-Algorithm code was written in \textbf{Python} with \textbf{PyTorch} and \textbf{NumPy} libraries, as well as functions \textit{deque()} and \textit{namedtuple()} from \textbf{collections} library. The \textit{Lunar Lander} environment was imported from \textit{Gym} library.

In the beginning, parameters set during each run were based on parameters found in code online \ref{}. After unsuccsessful runs these parameteres were randomly changed in order to improve agents behaviour, until the agent could solve the environment in 5 subsequent runs. Parameters used during these runs are tablified in \ref{tab:x} and will be later referred to as "standard parameters".

Once 5 successful runs were achieved, one of the parameters was changed and its influence on the score achieved by the agent and loss during next 5 runs was noted. Scores and loss were averaged across the whole set of these 5 runs and standard devation was calculated at each episode. Both score average and standard devation were plotted in a linear plot with shaded area errorbars using moving average of last 100 values (average score or loss) and standard deviations (shaded area). Plots were written with \textbf{Matplotlib} library in \textbf{Python}.

\begin{table}[h]
\caption{Parameters used resulting in first successful runs. Later referred to as "standard parameters".}
\label{tab:table1}
\begin{center}
    \begin{tabular}{cc}
        \hline
        \textbf{Parameter}                  & \textbf{Value} \\
        \hline
        Batch size                          & 100     \\
        Replay memory size                  & 100000  \\
        Target network update frequency     & 6       \\
        Learning rate (LR)                  & 2.5e-4  \\
        Discount factor $\gamma$            & 0.99    \\
        $\varepsilon$-start                 & 1       \\
        $\varepsilon$-decay                 & 0.995   \\
        $\varepsilon$-final                 & 0.01    \\
        $\tau$                              & 2.5e-3  \\
        Amount of hidden layers             & 2       \\
        Amount of neurons per hidden layer  & 64      \\
        \hline
    \end{tabular}
\end{center}
\end{table}

\pagebreak

\section{Reinforcement Learning}
Reinforcement Learning (RL) is a subset of Machine Learning (ML) which is concerned with usage of big datasets for training an AI agent.

RL assumes that every interaction between any agent and an environment can be modeled as maximisation of some kind of reward received by an agent due to interaction with its surroundings. The definition of the reward is ambiguous and its correct decription in code, which is at the hearth of RL reasearch, is the main task of programming using RL.
\subsection{Agent, States, Actions and Rewards}
TXT
\subsubsection{Section}
TXT

\subsection{Bellman equation and loss function}
Bellman equation describes the action-state function $Q^\pi$ based on the current state $s$ and possible actions $a$ to take at that state.

\begin{equation}
\label{eqn:bellman}
Q^\pi(s,a) = r+\gamma Q^\pi(s',\pi(s'))
\end{equation}
\\
Loss function $\mathcal{L}$ is a squared difference of Q-target and the current Q-value.

\begin{equation}
\label{eqn:loss}
\mathcal{L} = \alpha(r+\gamma\max Q(s',a')-Q(s,a))^2
\end{equation}

\section{Deep Q-Learning}
\subsection{Neural Networks - The \textit{Deep} of Q-Learning}
Deep Q-Learning combines the usage of action-state function $Q^\pi$ to describe the state, in which our agent resides, with neural networks processing the input data through a complicated network of layered neurons. At the end, the neural network outputs an "answer" to the current state.

\section{\textit{Lunar Lander} environment}
Until now we considered the technicalities and theory governing the algorithm of an agent. In this section we will go in depth into the implementation of DQN. This part of the research internship consisted of using previously gained theoretical knowdledge of the subject, aswell as online resources (such as tutorials and official documentation of libraries) and applying them in \textit{Python}. Here, using libraries \textit{PyTorch} and \textit{Gym} (as well as other smaller ones, such as e.g. Matplotlib, NumPy), I programmed an Deep Q-Algorithm that was able to interact with an online distributed Box2D environment  \textit{Lunar Lander} and solve it, i.e. achieve an average score of 200 in past 100 episodes.

\subsection{Description of the environment}
In the game of \textit{Lunar Lander} the task of the agent is to land a lunar lander onto an area in the middle of the screen (at coordinates $(0,0)$) in a given time without crashing the vehicle or leaving the bounds set by the environment. The environment is described with an vector with 8 variables (such as x position, y position, angle with respect to the ground, ...). Agent interacts with the environment through 4 actions; doing nothing or turning one of the three engines on (either left, right or bottom).

\subsection{Setting up the environoment}
Using command \textit{import gym} and env = gym.make("LunarLander-v2") the environment is imported into the IDE'\footnotemark \space workspace.
\footnotetext{IDE used was PyCharm Community Edition}

\subsubsection{Reward system}
Reward and the choices of the agent are based on the action-value function \ref{eqn:bellman}.

\subsubsection{Defining the neural network \textit{class}}
The neural network was divided

\newpage
\section*{Results and Discussion}
During the first consistent and successful runs the agent was able to solve the environment from 800 to 1200 episodes.

\subsection*{Behaviours of the agent during training}
In order to solve the environment and maximize the reward from given episode the agent tried multiple tactics; (a) hovering with the lunar lander directly or high above the landing area, then slowly descending onto the designated area, (b) dropping fast towards the ground while not breaking the lander, then using one of the engines on the side to push the lander towards the landing area and (c) the most resembling human-like play-style, slowing down from the initial state and descending in a swing-like motion towards the landing area.

During multiple training cycles the agent showed similar behaviours, which resulted in achieving lower scores, such as not slowing down enough from the starting condition and crashing the lander into the ground, rolling the lander over, not solving the environment in given time (steps), not turning the engines off while on the landing platform or getting out of environments bounds, thus either finishing the episode (in the case of crashing and rolling over) or achieving a huge negative score.

\subsection*{Importing of state dictionary from previous runs}
Importing state dictionaries from neural networks of trained agents to an untrained agent significantly increased scores gained by the untrained agent during the run (all parameters during both sets of runs remained standard with amount of hidden layers set to 3). The agent solved the environment in a much smaller amount of episodes (around 450) then during runs where agent had to learn the environment by itself. A huge spike in loss function at the begining of the run using imported state dictionary, with steady decrease afterwards, can be observed. This spike is caused by the difference between local and target networks in the begining of the run.

FIGURE

\subsection*{Amount of hidden layers}
The amount of hidden layers was changed to 4 and 5 with all other variables remaining standard. Best results were achieved using 4 hidden layers; agent solved the environment in 1020 episodes on average. Worst results were achieved using 5 hidden layers. The amount of hidden layers had no influence on loss measured across the runs.

FIGURE
\pagebreak

\subsection*{Amount of neurons per hidden layer}
Amount of neurons was changed to powers of 2 (to 32, 64, 128, 256 neurons per hidden layer respectively, all other parameters standard). The amount of neurons per hidden layer had no influence on score distribution across the runs. The best results were achieved using 32 and 128 neurons per hidden layer; agent solved the environment in 525 episodes on average. The worst result was achieved using 256 neurons per hidden layer.

FIGURE

\graphicspath{C:/Users/kryst/Desktop/Studia/TUM/Bachelor CBT/WiSe 22-23/Forschungspraktikum RL/Final diagramms}

\subsection*{Value of $\varepsilon$-decay}
$\varepsilon$-decay was varied from 0.995 to 0.9975 and 0.999 (all other parameters standard, amount of hidden layers set to 3). Using $\varepsilon$-decay = 0.9975 was found to be the most optimal value across all runs. Using this value, agent was able to solve the environment in around 800 episodes on average. Second best value was $\varepsilon$-decay = 0.995 and the worst $\varepsilon$-decay = 0.999. Also, following relationship was observed: smaller $\varepsilon$-decay values tend to minimise the loss function faster during the run.

The middle value of $\varepsilon$-decay, compared to the other two, allowed the agent to explore the environment and try out new policies for long enough to learn them, whereas a smaller $\varepsilon$ quickly reached the $\varepsilon$-final resulting in agent sticking with high probability to already learned policy without exploring new ones, thus not improving the score for a long time. A similiar result was observed when $\varepsilon$-decay = 0.999 was used; here the agent didn't stick with the best policy, constantly trying out new ones, thus taking more episodes to solve the environment.

FIGURE

\subsection*{Learning rate}
Learning rate $lr$ of the agent was set to $1\cdot10^{-4}, 2.5\cdot10^{-4}$ and $5\cdot10^{-4}$ respectively with all other parameters remaining standard and amount of hidden layers set to 3. The best results were achieved with $lr = 5\cdot10^{-4}$; agent solved the environment in x episodes on average. The worst results were achieved using $lr = 5\cdot10^{-4}$; agent was able to solve the environment in 1 out of 5 runs in 1828 episodes.

Inability to solve the environment with smaller learning rates can be explained with high $\varepsilon$-decay and small $\varepsilon$-final value, which disallowed the agent to explore new possibilities within the environment. Because of that, the agent stuck with best policies learned up to 1000th episode, after which stopped accuquiring new experiences.

FIGURE

\end{document}